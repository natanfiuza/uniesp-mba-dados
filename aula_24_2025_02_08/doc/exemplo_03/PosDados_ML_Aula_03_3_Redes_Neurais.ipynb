{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwmHcqlDSP_h"
      },
      "source": [
        "## **Importação das bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras.utils\n",
        "!pip install np_utils"
      ],
      "metadata": {
        "id": "QE2-Vs70ohN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLzHifcfZAWg"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pandas.testing as tm\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import six\n",
        "from abc import ABCMeta\n",
        "from scipy import sparse\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "# from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "# from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers import Convolution1D\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7l2Sdh5Sa9M"
      },
      "source": [
        "## **Preparando as amostras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVx5ftrISpCp"
      },
      "source": [
        "### **Segmentação da Amostra**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Qr2NqU_LA6"
      },
      "source": [
        "data = pd.read_csv('/content/amazon.csv') #link: https://drive.google.com/open?id=1JmGqiLEh_wUeLilLF7Hv_w4jgUNbG0Q_\n",
        "data.head()\n",
        "\n",
        "data = data[:1000] #só para rodar mais rápido durante a aula\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub5s56Y1BakJ"
      },
      "source": [
        "data = data[data['Reviews'].isnull()==False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOkGtd2fBm34"
      },
      "source": [
        "train, test = train_test_split(data, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohfUT_ukBrEi"
      },
      "source": [
        "sns.countplot(data['Rating'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDPbhmFISifH"
      },
      "source": [
        "### **Pré-processamento do texto**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQTyfaAc5SIh"
      },
      "source": [
        "# Função para pré-processamento do texto\n",
        "# Converte o documento em uma sequencia de palavras e remove algumas stop words.\n",
        "# Retorna uma lista de palavras\n",
        "def review_to_wordlist( review, remove_stopwords=True):\n",
        "\n",
        "    # 1. Remover HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "\n",
        "    # 2. Remover caracteres numéricos\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
        "    #\n",
        "    # 3. Converte palavras para minúsculas e sem espaços\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Remover opcionalmente as stop words (True - default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    b=[]\n",
        "    stemmer = english_stemmer #PorterStemmer()\n",
        "    for word in words:\n",
        "        b.append(stemmer.stem(word))\n",
        "\n",
        "    # 5. Returna uma lista de palavras\n",
        "    return(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoZ54ARBv2T"
      },
      "source": [
        "nltk.download('stopwords') #Lista para palavras em inglês\n",
        "\n",
        "clean_train_reviews = []\n",
        "for review in train['Reviews']:\n",
        "    clean_train_reviews.append( \" \".join(review_to_wordlist(review)))\n",
        "\n",
        "clean_test_reviews = []\n",
        "for review in test['Reviews']:\n",
        "    clean_test_reviews.append( \" \".join(review_to_wordlist(review)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3GP__SdByGU"
      },
      "source": [
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ), sublinear_tf = True )\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_reviews)\n",
        "train_features = vectorizer.transform(clean_train_reviews)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvSZ6IISByuC"
      },
      "source": [
        "fselect = SelectKBest(chi2 , k=100) #10000. #Select features according to the k highest scores.\n",
        "train_features = fselect.fit_transform(train_features, train[\"Rating\"])\n",
        "test_features = fselect.transform(test_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PICu7RjZB4Us"
      },
      "source": [
        "## **Outros Modelos - SGD, Random Forest, Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5HMMOoQBy46"
      },
      "source": [
        "model1 = MultinomialNB(alpha=0.001) #Naive Bayes\n",
        "model1.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model2 = SGDClassifier(loss='modified_huber', random_state=0, shuffle=True) #outra variação do SVM\n",
        "model2.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model3 = RandomForestClassifier()\n",
        "model3.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "model4 = GradientBoostingClassifier()\n",
        "model4.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "pred_1 = model1.predict( test_features.toarray() )\n",
        "pred_2 = model2.predict( test_features.toarray() )\n",
        "pred_3 = model3.predict( test_features.toarray() )\n",
        "pred_4 = model4.predict( test_features.toarray() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-nSfYyqB8Qs"
      },
      "source": [
        "#Implementando uma Naive Bayes\n",
        "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
        "\n",
        "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.C = C\n",
        "        self.svm_ = [] # fuggly\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y, 'csr')\n",
        "        _, n_features = X.shape\n",
        "\n",
        "        labelbin = LabelBinarizer()\n",
        "        Y = labelbin.fit_transform(y)\n",
        "        self.classes_ = labelbin.classes_\n",
        "        if Y.shape[1] == 1:\n",
        "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
        "\n",
        "        Y = Y.astype(np.float64)\n",
        "\n",
        "        # Contagem dos registros dos dados\n",
        "        n_effective_classes = Y.shape[1]\n",
        "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
        "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
        "                                 dtype=np.float64)\n",
        "        self._compute_ratios(X, Y)\n",
        "\n",
        "        # flugglyness\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
        "            Y_i = Y[:,i]\n",
        "            svm.fit(X_i, Y_i)\n",
        "            self.svm_.append(svm)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_effective_classes = self.class_count_.shape[0]\n",
        "        n_examples = X.shape[0]\n",
        "\n",
        "        D = np.zeros((n_effective_classes, n_examples))\n",
        "\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            D[i] = self.svm_[i].decision_function(X_i)\n",
        "\n",
        "        return self.classes_[np.argmax(D, axis=0)]\n",
        "\n",
        "    def _compute_ratios(self, X, Y):\n",
        "        '''Contar ocorrências de recursos e proporções de computação.'''\n",
        "        if np.any((X.data if issparse(X) else X) < 0):\n",
        "            raise ValueError(\"Entrada X deveria ser positiva\")\n",
        "\n",
        "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
        "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
        "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
        "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
        "        check_array(self.ratios_)\n",
        "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
        "\n",
        "\n",
        "def f1_class(pred, truth, class_val):\n",
        "    n = len(truth)\n",
        "\n",
        "    truth_class = 0\n",
        "    pred_class = 0\n",
        "    tp = 0\n",
        "\n",
        "    for ii in range(0, n):\n",
        "        if truth[ii] == class_val:\n",
        "            truth_class += 1\n",
        "            if truth[ii] == pred[ii]:\n",
        "                tp += 1\n",
        "                pred_class += 1\n",
        "                continue;\n",
        "        if pred[ii] == class_val:\n",
        "            pred_class += 1\n",
        "\n",
        "    precision = tp / float(pred_class)\n",
        "    recall = tp / float(truth_class)\n",
        "\n",
        "    return (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def semeval_senti_f1(pred, truth, pos=2, neg=0):\n",
        "\n",
        "    f1_pos = f1_class(pred, truth, pos)\n",
        "    f1_neg = f1_class(pred, truth, neg)\n",
        "\n",
        "    return (f1_pos + f1_neg) / 2.0;\n",
        "\n",
        "\n",
        "def main(train_file, test_file, ngram=(1, 3)):\n",
        "    print('carregando...')\n",
        "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    print('vetorizando...')\n",
        "    vect = CountVectorizer()\n",
        "    classifier = NBSVM()\n",
        "\n",
        "    # create pipeline\n",
        "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
        "    params = {\n",
        "        'vect__token_pattern': r\"\\S+\",\n",
        "        'vect__ngram_range': ngram,\n",
        "        'vect__binary': True\n",
        "    }\n",
        "    clf.set_params(**params)\n",
        "\n",
        "    print('treinando...')\n",
        "    clf.fit(train['text'], train['label'])\n",
        "\n",
        "    print('classificando...')\n",
        "    pred = clf.predict(test['text'])\n",
        "\n",
        "    print('teste...')\n",
        "    acc = accuracy_score(test['label'], pred)\n",
        "    f1 = semeval_senti_f1(pred, test['label'])\n",
        "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))\n",
        "\n",
        "\n",
        "model5 = NBSVM(C=0.01)\n",
        "model5.fit( train_features, train[\"Rating\"] )\n",
        "\n",
        "pred_5 = model5.predict( test_features )\n",
        "pred_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC6jMyofCH0G"
      },
      "source": [
        "print(classification_report(test['Rating'], pred_2, target_names=['1','2','3','4','5']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKzOxvMxCKAQ"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Matriz de confusão',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    '''\n",
        "     Esta função imprime e plota a matriz de confusão.\n",
        "     A normalização pode ser aplicada configurando `normalize = True`.\n",
        "    '''\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matriz de confusão normalizada\")\n",
        "    else:\n",
        "        print('Matriz de confusão, não normalizada')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Calculando a matriz de confusão\n",
        "cnf_matrix = confusion_matrix(test['Rating'], pred_5)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(cnf_matrix, classes=['1','2','3','4','5'],title='Matriz de confusão, sem normalização')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RbFaoECPlZ"
      },
      "source": [
        "print('predição 1 - accuracy: ', accuracy_score(test['Rating'], pred_1))\n",
        "print('predição 2 - accuracy: ', accuracy_score(test['Rating'], pred_2))\n",
        "print('predição 3 - accuracy: ', accuracy_score(test['Rating'], pred_3))\n",
        "print('predição 4 - accuracy: ', accuracy_score(test['Rating'], pred_4))\n",
        "print('predição 5 - accuracy: ', accuracy_score(test['Rating'], pred_5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61WjQjxCSvW"
      },
      "source": [
        "## **Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfkYVAmZLwTg"
      },
      "source": [
        "### **Modelo MLP**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX1BYfk4CXQO"
      },
      "source": [
        "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 1000, ngram_range = ( 1, 3 ),sublinear_tf = True )\n",
        "\n",
        "vectorizer = vectorizer.fit(clean_train_reviews)\n",
        "train_features = vectorizer.transform(clean_train_reviews)\n",
        "\n",
        "test_features = vectorizer.transform(clean_test_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxhdvWPrCUoP"
      },
      "source": [
        "batch_size = 32 #Tamanho do lote (Batch Size)\n",
        "nb_classes = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyB2-kVJtXQL"
      },
      "source": [
        "X_train = train_features.toarray()\n",
        "X_test = test_features.toarray()\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "y_train = np.array(train['Rating']-1)\n",
        "y_test = np.array(test['Rating']-1)\n",
        "\n",
        "#Criacao dos Modelos com SKlearn\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
        "\n",
        "#treinamento\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#teste\n",
        "y_prev = clf.predict(X_test)\n",
        "\n",
        "print('predição MLP (SKLearn) - accuracy: ', accuracy_score(y_test, y_prev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqxskbVMCXT0"
      },
      "source": [
        "#MLP com Keras\n",
        "\n",
        "X_train = train_features.toarray()\n",
        "X_test = test_features.toarray()\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "y_train = np.array(train['Rating']-1)\n",
        "y_test = np.array(test['Rating']-1)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "# pre-processamento: dividindo pelo max e pela subtração da média\n",
        "scale = np.max(X_train)\n",
        "X_train /= scale\n",
        "X_test /= scale\n",
        "\n",
        "mean = np.mean(X_train)\n",
        "X_train -= mean\n",
        "X_test -= mean\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Criação do Modelo de Deep Dumb MLP (DDMLP)\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=input_dim)) #camada 1 256 neurônio #input_dim\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128))  #camada 2 - 128 neurônios\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(nb_classes)) #camada 3 - saida\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# usaremos xent categórico para a perda e o RMSprop como otimizador\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "#Treinamento\n",
        "model.fit(X_train, Y_train, epochs=5, batch_size=16, validation_split=0.1)\n",
        "\n",
        "#Testando o modelo\n",
        "preds_x = model.predict(X_test)\n",
        "preds = np.argmax(preds_x, axis=1)\n",
        "print('Predição MLP (Keras) -accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8JnVwmJClCS"
      },
      "source": [
        "### **Modelo LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6sVPFyUCiNe"
      },
      "source": [
        "max_features = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "maxlen = 70\n",
        "batch_size = 32\n",
        "nb_classes = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTG7aPK1CwPQ"
      },
      "source": [
        "# vetorizando as amostras de texto em um tensor inteiro 2D\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(train['Reviews'])\n",
        "sequences_train = tokenizer.texts_to_sequences(train['Reviews'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test['Reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm8pGDAbCw2m"
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DV43U1-CzCB"
      },
      "source": [
        "#Construindo o Modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(nb_classes)) #camada de saida\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#treinamento\n",
        "print('Treinamento...')\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=1,validation_data=(X_test, Y_test))\n",
        "\n",
        "#testando o modelo\n",
        "score, acc = model.evaluate(X_test, Y_test,batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Gerando as Previsões...\")\n",
        "preds_x = model.predict(X_test)\n",
        "preds = np.argmax(preds_x, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnzg0JBJC4Oj"
      },
      "source": [
        "print('prediction 7 - accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayguHb69C43P"
      },
      "source": [
        "### **Modelo ConveNet - CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rD6CnjsC535"
      },
      "source": [
        "nb_filter = 32\n",
        "filter_length = 3\n",
        "hidden_dims = 250\n",
        "nb_epoch = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfAwNE0eC8PB"
      },
      "source": [
        "#Criacao do Modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "\n",
        "# nós adicionamos um Convolution1D, que aprenderá nb_filter\n",
        "# Grupo de palavras filtradas pelo filter_length:\n",
        "model.add(Convolution1D(32, 3,\n",
        "                        padding='valid',\n",
        "                        activation='relu'))\n",
        "\n",
        "def max_1d(X):\n",
        "    return K.max(X, axis=1)\n",
        "\n",
        "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MeaelYj0Rxf"
      },
      "source": [
        "#treinamento\n",
        "print('Treinamento...')\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=1,validation_data=(X_test, Y_test))\n",
        "\n",
        "#testando o modelo\n",
        "score, acc = model.evaluate(X_test, Y_test,batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Gerando as Previsões...\")\n",
        "preds_x = model.predict(X_test)\n",
        "preds = np.argmax(preds_x, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZO901I0G7u"
      },
      "source": [
        "print('prediction 8 - accuracy: ', accuracy_score(test['Rating'], preds+1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}