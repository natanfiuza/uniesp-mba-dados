#  Regress√£o Log√≠stica

Aprender como modelar e prever *probabilidades* e *categorias*!

## Introdu√ß√£o

At√© agora, focamos em problemas de regress√£o onde a vari√°vel dependente era *cont√≠nua* (pre√ßo de uma casa, nota de um aluno). Mas e quando queremos prever algo que √© *categ√≥rico*? Por exemplo:

*   Um cliente vai comprar um produto (sim/n√£o)?
*   Um email √© spam (spam/n√£o spam)?
*   Um paciente tem uma determinada doen√ßa (doente/saud√°vel)?
*   Qual o tipo de uma imagem, dentre 3 categorias (cachorro/gato/p√°ssaro)

Nestes casos, a Regress√£o Linear n√£o √© a ferramenta adequada. Precisamos de um modelo que preveja a *probabilidade* de um evento ocorrer (ou de uma observa√ß√£o pertencer a uma determinada categoria). √â aqui que entra a Regress√£o Log√≠stica!

## Conceitos B√°sicos

### 1.  **Vari√°vel Dependente Categ√≥rica** 

A vari√°vel que queremos prever assume um n√∫mero limitado de valores (categorias). Pode ser:

    *   **Bin√°ria:** Duas categorias (sim/n√£o, 0/1).
    *   **Multinomial:** Mais de duas categorias (cachorro/gato/p√°ssaro).

### 2.  **Fun√ß√£o Log√≠stica (Sigmoide)** 

A Regress√£o Log√≠stica utiliza uma fun√ß√£o especial chamada *fun√ß√£o log√≠stica* ou *sigmoide* para transformar uma combina√ß√£o linear das vari√°veis independentes em uma probabilidade (um valor entre 0 e 1). A equa√ß√£o da fun√ß√£o sigmoide √©:

    `P(Y=1) = 1 / (1 + e^(-z))`

    Onde:

    *   `P(Y=1)` √© a probabilidade de a vari√°vel dependente ser igual a 1 (a categoria de interesse).
    *   `e` √© a base do logaritmo natural (aproximadamente 2.718).
    *   `z` √© a combina√ß√£o linear das vari√°veis independentes: `z = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ... + Œ≤‚ÇôX‚Çô`
        *   `Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô` s√£o os coeficientes do modelo (semelhantes aos da regress√£o linear).
        *   `X‚ÇÅ, X‚ÇÇ, ..., X‚Çô` s√£o as vari√°veis independentes.

### 3.  **Interpreta√ß√£o dos Coeficientes** 

Ao contr√°rio da regress√£o linear, os coeficientes na regress√£o log√≠stica *n√£o* representam diretamente a mudan√ßa na vari√°vel dependente. Em vez disso, eles afetam o *log-odds* (logaritmo da raz√£o de chances).

    *   **Odds (Raz√£o de Chances):** A raz√£o entre a probabilidade de um evento ocorrer e a probabilidade de ele n√£o ocorrer: `odds = P(Y=1) / (1 - P(Y=1))`.
    *   **Log-Odds:** O logaritmo natural das odds: `log(odds) = z = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + ...`.
    *   Um coeficiente positivo (`Œ≤·µ¢ > 0`) significa que um aumento na vari√°vel independente `X·µ¢` aumenta o log-odds, e portanto aumenta a probabilidade de `Y=1`.
    *   Um coeficiente negativo (`Œ≤·µ¢ < 0`) significa que um aumento em `X·µ¢` diminui o log-odds e a probabilidade de `Y=1`.
    *   Um coeficiente igual a zero (`Œ≤·µ¢ = 0`) indica que a vari√°vel `X·µ¢` n√£o tem efeito sobre a probabilidade.
    * Para ter uma interpreta√ß√£o mais direta, pode-se usar a exponencial dos coeficientes (`exp(Œ≤·µ¢)`), que representa a mudan√ßa *multiplicativa* nas odds para cada unidade de aumento em `X·µ¢`.

### 4. **Estimativa dos Coeficientes (M√°xima Verossimilhan√ßa)** 

Diferente da regress√£o linear (que usa m√≠nimos quadrados), a regress√£o log√≠stica usa um m√©todo chamado *M√°xima Verossimilhan√ßa* (Maximum Likelihood Estimation - MLE). O MLE busca os coeficientes que maximizam a probabilidade de observar os dados que realmente observamos. N√£o h√° uma f√≥rmula fechada como nos m√≠nimos quadrados; o MLE √© resolvido iterativamente por algoritmos de otimiza√ß√£o.

### 5. **Classifica√ß√£o** 

Uma vez que temos as probabilidades estimadas, podemos classificar as observa√ß√µes. Normalmente, usamos um *limiar de decis√£o* (threshold). Por exemplo, se a probabilidade estimada `P(Y=1)` for maior ou igual a 0.5, classificamos a observa√ß√£o como pertencente √† categoria 1 (sim, doente, spam, etc.). Caso contr√°rio, classificamos como categoria 0.

## Exemplos Pr√°ticos

### **Exemplo 1: Aprova√ß√£o em um Exame**

Queremos prever se um aluno ser√° aprovado em um exame (sim/n√£o) com base em suas horas de estudo.

| Horas de Estudo | Aprovado (1=Sim, 0=N√£o) |
| --------------- | ----------------------- |
| 2               | 0                       |
| 4               | 0                       |
| 6               | 1                       |
| 8               | 1                       |
| 10              | 1                       |

### **Exemplo 2: Risco de Doen√ßa Card√≠aca**

Queremos modelar a probabilidade de uma pessoa ter doen√ßa card√≠aca com base em fatores de risco como idade, colesterol, press√£o arterial, etc. A vari√°vel dependente √© bin√°ria (doente/saud√°vel).

## Ferramentas

*   **Python:**
    *   `scikit-learn`: `LogisticRegression`.
    *   `statsmodels`: `Logit` (fornece mais detalhes estat√≠sticos).

*   **R:**
    *   `glm()` com `family = binomial(link = "logit")`.

## Casos de Uso

A regress√£o log√≠stica √© amplamente utilizada em diversas √°reas:

1.  **Medicina:** Diagn√≥stico de doen√ßas, predi√ß√£o de risco de mortalidade, efic√°cia de tratamentos.
2.  **Marketing:** Previs√£o de churn (cancelamento de clientes), resposta a campanhas de marketing, propens√£o a comprar.
3.  **Finan√ßas:** An√°lise de risco de cr√©dito, detec√ß√£o de fraudes.
4.  **Ci√™ncias Sociais:** Modelagem de comportamento eleitoral, an√°lise de risco de criminalidade.
5.  **Processamento de Linguagem Natural:** Classifica√ß√£o de texto (an√°lise de sentimento, detec√ß√£o de spam).
6.  **Vis√£o Computacional:** Classifica√ß√£o de imagens (embora redes neurais profundas sejam mais comuns hoje em dia).

## Regress√£o Log√≠stica Multinomial

Quando a vari√°vel dependente tem mais de duas categorias, usamos a *Regress√£o Log√≠stica Multinomial* (tamb√©m conhecida como *Softmax Regression*). Em vez de uma √∫nica fun√ß√£o sigmoide, usamos a fun√ß√£o *softmax*, que generaliza a sigmoide para m√∫ltiplas categorias. A softmax produz um vetor de probabilidades, uma para cada categoria, e a soma dessas probabilidades √© igual a 1.

## Avalia√ß√£o do Modelo

Como a regress√£o log√≠stica lida com classifica√ß√£o, usamos m√©tricas diferentes daquelas da regress√£o linear (R¬≤, RMSE). Algumas m√©tricas comuns:

*   **Acur√°cia:** Propor√ß√£o de classifica√ß√µes corretas.
*   **Precis√£o:** Propor√ß√£o de verdadeiros positivos entre todos os classificados como positivos (qu√£o "preciso" √© o modelo ao prever a classe positiva).
*   **Recall (Revoca√ß√£o/Sensibilidade):** Propor√ß√£o de verdadeiros positivos entre todos os que *realmente* s√£o positivos (qu√£o "completo" √© o modelo ao identificar a classe positiva).
*   **F1-Score:** M√©dia harm√¥nica entre precis√£o e recall.
*   **Curva ROC e AUC:** Medem a capacidade do modelo de distinguir entre as classes em diferentes limiares de decis√£o.
*   **Matriz de Confus√£o:** Tabela que mostra os resultados da classifica√ß√£o (verdadeiros positivos, falsos positivos, verdadeiros negativos, falsos negativos).

## Pr√≥ximos Passos

Agora que voc√™ conhece a Regress√£o Log√≠stica, pode aprofundar seus conhecimentos:

*   **Regulariza√ß√£o (L1/Lasso e L2/Ridge):** Essencial para evitar overfitting, especialmente com muitas vari√°veis independentes.
*   **Sele√ß√£o de Vari√°veis:** M√©todos para escolher as vari√°veis mais relevantes para o modelo.
*   **Desbalanceamento de Classes:** T√©cnicas para lidar com situa√ß√µes em que uma categoria √© muito mais frequente que a outra.
*   **Compara√ß√£o com outros modelos de classifica√ß√£o:** √Årvores de decis√£o, Random Forests, Support Vector Machines (SVMs), Redes Neurais.

A Regress√£o Log√≠stica √© uma ferramenta poderosa e vers√°til para problemas de classifica√ß√£o. Domin√°-la abre muitas portas no mundo do Machine Learning!

[üîô Voltar ](./fundamentos_regressao.md)